{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c441095e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install datasets transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdbac3a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# some may be extra\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, DataCollatorForTokenClassification, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from transformers import AutoModel, AutoConfig\n",
    "from tokenizers import normalizers\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import sigmoid\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5de6f0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model_checkpoint = \"you/your-model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True, add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe872850",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def make_last_subtoken_mask(mask, has_cls=True, has_eos=True):\n",
    "    if has_cls:\n",
    "        mask = mask[1:]\n",
    "    if has_eos:\n",
    "        mask = mask[:-1]\n",
    "    is_last_word = list((first != second) for first, second in zip(mask[:-1], mask[1:])) + [True]\n",
    "    if has_cls:\n",
    "        is_last_word = [False] + is_last_word\n",
    "    if has_eos:\n",
    "        is_last_word.append(False)\n",
    "    return is_last_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23950a2e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# collecting tasks from train subset\n",
    "all_tasks = {\"POS\"}\n",
    "with open('train.conllu', \"r\", encoding=\"utf8\") as fin:\n",
    "        for line in fin:\n",
    "            line = line.strip()\n",
    "            if line == \"\" or not line[0].isdigit():\n",
    "                continue\n",
    "            splitted = line.split(\"\\t\")\n",
    "            feats = splitted[5]\n",
    "            if feats != \"_\":\n",
    "                for feat in feats.split(\"|\"):\n",
    "                        key, _ = feat.split(\"=\")\n",
    "                        all_tasks.add(key)\n",
    "task_names = sorted(all_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07417cd4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# reading conll-u\n",
    "def read_mt(infile):\n",
    "    answer, sentences = [], []\n",
    "    with open(infile, \"r\", encoding=\"utf8\") as fin:\n",
    "        sent = []\n",
    "        labels = {task: [] for task in task_names}\n",
    "        for line in fin:\n",
    "            line = line.strip()\n",
    "            if line == \"\":\n",
    "                if sent:\n",
    "                    answer.append({\"words\": sent, \"labels\": {k: v[:] for k, v in labels.items()}})\n",
    "                sent = []\n",
    "                labels = {task: [] for task in task_names}\n",
    "                continue\n",
    "\n",
    "            splitted = line.split(\"\\t\")\n",
    "            if not splitted[0].isdigit():\n",
    "                continue\n",
    "            sent.append(splitted[1])\n",
    "            pos_tag, feats = splitted[3], splitted[5]\n",
    "            labels[\"POS\"].append(pos_tag)\n",
    "            feats_dict = {}\n",
    "            if feats != \"_\":\n",
    "                for feat in feats.split(\"|\"):\n",
    "                        key, val = feat.split(\"=\")\n",
    "                        feats_dict[key] = val\n",
    "\n",
    "            for task in task_names:\n",
    "                if task != \"POS\" and task!= \"UD-feats\":\n",
    "                    labels[task].append(feats_dict.get(task, \"None\"))\n",
    "\n",
    "        if sent: #processing the last string\n",
    "            answer.append({\"words\": sent, \"labels\": {k: v[:] for k, v in labels.items()}})\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc7601d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "train_mt = read_mt('train.conllu')\n",
    "for k, v in train_mt[1].items(): # to see how the data looks after being read\n",
    "    print(k, v)\n",
    "eval_mt = read_mt('dev.conllu')\n",
    "test_mt = read_mt('test.conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd2f6be",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class MultiTaskUDDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, min_count=1, tags=None): \n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.ignore_index = -100\n",
    "        # extracting tasks from the first item\n",
    "        self.tasks = list(data[0][\"labels\"].keys())\n",
    "\n",
    "        # tag dictionary for each task\n",
    "        self.tags_ = {}\n",
    "        self.tag_indexes_ = {}\n",
    "        for task in self.tasks:\n",
    "            if tags is None or task not in tags:\n",
    "                tag_counts = Counter([label for item in data for label in item[\"labels\"][task]])\n",
    "                task_tags = [x for x, count in tag_counts.items() if count >= min_count]\n",
    "            else:\n",
    "                task_tags = tags[task]\n",
    "            self.tags_[task] = task_tags\n",
    "            self.tag_indexes_[task] = {tag: i for i, tag in enumerate(task_tags)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "      item = self.data[index]\n",
    "      tokenization = self.tokenizer(item[\"words\"], is_split_into_words=True)\n",
    "      last_subtoken_mask = make_last_subtoken_mask(tokenization.word_ids())\n",
    "      input_ids = tokenization[\"input_ids\"]\n",
    "      answer = {\"input_ids\" : input_ids}\n",
    "\n",
    "      if \"labels\" in item:\n",
    "        labels_out = {}\n",
    "        for task in self.tasks:\n",
    "          labels = [self.tag_indexes_[task][label] for label in item[\"labels\"][task]]\n",
    "          zero_labels = np.array([self.ignore_index] * len(input_ids), dtype=int) # -100 initialization\n",
    "          zero_labels[last_subtoken_mask] = labels # labels are assigned to the last subtokens\n",
    "          labels_out[task] = zero_labels\n",
    "        answer[\"labels\"] = labels_out\n",
    "      return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3cc8b6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "train_ds = MultiTaskUDDataset(train_mt, tokenizer=tokenizer)\n",
    "eval_ds = MultiTaskUDDataset(eval_mt, tokenizer=tokenizer, tags = train_ds.tags_)\n",
    "test_ds = MultiTaskUDDataset(test_mt, tokenizer=tokenizer, tags=train_ds.tags_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3bf082",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# to see how the data has changed\n",
    "print(eval_mt[2])\n",
    "for k, v in eval_ds[2].items():\n",
    "    print(k)\n",
    "    if type(v) == dict:\n",
    "        for kk, vv in v.items():\n",
    "          print(kk, vv)\n",
    "    else:\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0ed98a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "num_labels_dict = {task: len(train_ds.tags_[task]) for task in train_ds.tasks}\n",
    "\n",
    "print(num_labels_dict)\n",
    "print(train_ds.tag_indexes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681ca14e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class MultiTaskDataCollator(DataCollatorWithPadding):\n",
    "    def __call__(self, features):\n",
    "        labels_dict = {}\n",
    "        for task_name in features[0][\"labels\"]:\n",
    "            labels_dict[task_name] = []\n",
    "        for feature in features:\n",
    "            for task_name, task_labels in feature.pop(\"labels\").items():\n",
    "                labels_dict[task_name].append(task_labels)\n",
    "\n",
    "        # input_ids, attention_mask, token_type_ids padding\n",
    "        batch = super().__call__(features)\n",
    "\n",
    "        # label padding for each task\n",
    "        if labels_dict is not None:\n",
    "            batch_labels = {}\n",
    "            max_length = batch[\"input_ids\"].shape[1]\n",
    "            for task_name, task_labels in labels_dict.items():\n",
    "                padded_task_labels = []\n",
    "                for label in task_labels:\n",
    "                    label = np.array(label)\n",
    "                    padding_length = max_length - label.shape[0]\n",
    "                    if padding_length > 0:\n",
    "                        padded_label = np.pad(label, (0, padding_length), constant_values=-100)\n",
    "                    else:\n",
    "                        padded_label = label\n",
    "                    padded_task_labels.append(padded_label)\n",
    "                batch_labels[task_name] = torch.tensor(\n",
    "                    np.array(padded_task_labels), dtype=torch.long\n",
    "                )\n",
    "            batch[\"labels\"] = batch_labels\n",
    "\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23beec6d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class AutoModelForMultiTaskTokenClassification(nn.Module):\n",
    "    def __init__(self, model_name, num_labels_dict):\n",
    "        super().__init__()\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        hidden_size = self.model.config.hidden_size\n",
    "        # a classifier for each task\n",
    "        self.classifiers = nn.ModuleDict({\n",
    "            task: nn.Linear(hidden_size, num_labels)\n",
    "            for task, num_labels in num_labels_dict.items()\n",
    "        })\n",
    "        self.loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        sequence_output = outputs.last_hidden_state  # (batch, seq_len, hidden)\n",
    "        logits, losses = {}, {}\n",
    "\n",
    "        for task, classifier in self.classifiers.items():\n",
    "            task_logits = classifier(sequence_output)  # (batch, seq, num_labels)\n",
    "            logits[task] = task_logits\n",
    "\n",
    "            if labels is not None and task in labels:\n",
    "                # CrossEntropyLoss <- (batch*seq, num_labels)\n",
    "                loss = self.loss_fct(\n",
    "                     task_logits.view(-1, task_logits.size(-1)),\n",
    "                     labels[task].view(-1)\n",
    "                )\n",
    "                losses[task] = loss\n",
    "\n",
    "        if labels is not None:\n",
    "            total_loss = sum(losses.values())/len(losses.keys())\n",
    "            return {\n",
    "                \"loss\": total_loss,\n",
    "                \"logits\": logits\n",
    "            }\n",
    "\n",
    "        return {\"logits\": logits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bf2c59",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def compute_metrics_with_tasks(eval_pred):\n",
    "    preds, labels = eval_pred.predictions, eval_pred.label_ids\n",
    "    task_names = preds.keys()\n",
    "\n",
    "    correct, total, seq_correct = 0, 0, 0\n",
    "    task_correct = {t: 0 for t in task_names}\n",
    "    task_total   = {t: 0 for t in task_names}\n",
    "    task_seq_correct = {t: 0 for t in task_names}\n",
    "\n",
    "    batch_size = list(preds.values())[0].shape[0]\n",
    "    seq_len = list(preds.values())[0].shape[1]\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        is_correct_seq = True\n",
    "        is_correct_seq_by_task = {t: True for t in task_names}\n",
    "        for t in range(seq_len):\n",
    "            is_correct_token, is_real_token = True, False\n",
    "            for task in task_names:\n",
    "                task_logits = preds[task]\n",
    "                task_labels = labels[task]\n",
    "                label = task_labels[i, t]\n",
    "                if label == -100:\n",
    "                    continue\n",
    "                else:\n",
    "                    is_real_token = True\n",
    "                pred = np.argmax(task_logits[i, t])\n",
    "                task_total[task] += 1\n",
    "                if pred != label:\n",
    "                    is_correct_token, is_correct_seq = False, False\n",
    "                    is_correct_seq_by_task[task] = False\n",
    "                else:\n",
    "                    task_correct[task] += 1\n",
    "\n",
    "            if is_real_token:\n",
    "                total += 1\n",
    "                correct += int(is_correct_token)\n",
    "        # sentence-level\n",
    "        seq_correct += int(is_correct_seq)\n",
    "        for task in task_names:\n",
    "                task_seq_correct[task] += int(is_correct_seq_by_task[task])\n",
    "\n",
    "    metrics = {}\n",
    "    metrics[\"token_acc\"] = 100 * correct / total\n",
    "    metrics[\"sent_acc\"] = 100 * seq_correct / batch_size\n",
    "\n",
    "    for task in task_names:\n",
    "        metrics[f\"{task}_acc\"] = 100 * task_correct[task] / task_total[task]\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366265e5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model = AutoModelForMultiTaskTokenClassification(model_checkpoint, num_labels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93e91a3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    num_train_epochs=5,\n",
    "    learning_rate = 5e-5,\n",
    "    eval_strategy = 'steps',\n",
    "    eval_steps = 200,\n",
    "    weight_decay = 0.01,\n",
    "    per_device_train_batch_size = 8,\n",
    "    per_device_eval_batch_size = 8,\n",
    "    report_to=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b554efb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    optimizers=(AdamW(model.parameters(), lr=5e-5, weight_decay=0.01), None),\n",
    "    compute_metrics=compute_metrics_with_tasks,\n",
    "    data_collator=MultiTaskDataCollator(tokenizer),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb87c1e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3306902",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# to fully print the metrics at the end\n",
    "predictions_train = trainer.predict(train_ds)\n",
    "predictions_eval = trainer.predict(eval_ds)\n",
    "predictions_test = trainer.predict(test_ds)\n",
    "\n",
    "rows = []\n",
    "for (tr_k, tr_v), (v_k, v_v), (t_k, t_v) in zip(predictions_train.metrics.items(), predictions_eval.metrics.items(), predictions_test.metrics.items()):\n",
    "    rows.append({\"task\": tr_k, \"train\": tr_v, \"eval\": v_v, \"test\": t_v})\n",
    "df = pd.DataFrame(rows)[1:-3]\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
